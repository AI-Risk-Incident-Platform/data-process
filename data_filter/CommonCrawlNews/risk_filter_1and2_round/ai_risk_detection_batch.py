#è¿™ä¸ªæ–‡ä»¶æ˜¯ç”¨æ¥æ‰¹é‡ç­›é€‰AIé£é™©æ–°é—»çš„
#é€šè¿‡è°ƒç”¨qwen2.5æ¨¡å‹è¿›è¡ŒAIé£é™©å…³é”®è¯ç¬¬ä¸€è½®ç­›é€‰ï¼Œå°†å¯¼å…¥åº“çš„æ–‡ä»¶æ¥æºæ”¹ä¸ºcallmodelå³å¯å®ç°ç¬¬ä¸€è½®ç­›é€‰æ“ä½œ
#é€šè¿‡è°ƒç”¨qwen3æ¨¡å‹è¿›è¡ŒAIé£é™©å…³é”®è¯ç¬¬äºŒè½®ç­›é€‰
#æ³¨æ„è°ƒç”¨æ¨¡å‹çš„æ–¹å¼
#è¿™é‡Œæ˜¯è°ƒç”¨æœ¬åœ°æ¨¡å‹çš„æ–¹å¼
#æ³¨æ„ä¿®æ”¹ç›¸å…³çš„è·¯å¾„å’Œé…ç½®
#è¯·ç¡®ä¿å·²ç»å®‰è£…äº†æ‰€éœ€çš„åº“å’Œæ¨¡å‹
import os
import time
import json
import logging
import asyncio
import random
from pathlib import Path
from bs4 import BeautifulSoup
import numpy as np
from callmodel_qwen3 import call_model
#from callmodel import call_model
import re
# ğŸš€ å¿…é¡»åœ¨å¯¼å…¥ torch å’Œ transformers ä¹‹å‰è®¾ç½®
#os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # åªç”¨ GPU 0
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"  # å‡å°‘æ˜¾å­˜ç¢ç‰‡
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # å…³é—­ tokenizer å¹¶è¡Œè­¦å‘Š
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

BASE_INPUT_DIR = "BASE_INPUT_DIR"  # æ›¿æ¢ä¸ºå®é™…çš„è¾“å…¥ç›®å½•
BASE_OUTPUT_DIR = "BASE_OUTPUT_DIR"  # æ›¿æ¢ä¸ºå®é™…çš„è¾“å‡ºç›®å½•

# === æ—¥å¿—è®¾ç½® ===
LOG_FILE = "batch_processing.log" # æ›¿æ¢ä¸ºå®é™…çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„
ERROR_LOG_FILE = "error.log"
#BATCH_SIZE = 10
CONTENT_LIMIT = 10000
SELECTED_YM = ["2022/5","2022/6","2022/7","2022/8","2022/9","2022/10","2022/11","2022/12"]  # é€‰æ‹©è¦å¤„ç†çš„å¹´æœˆ
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# === å…¨å±€æ¨ç†è®¡æ•°å™¨ ===
global_inference_counter = 0

# è®¾ç½® logger
logger = logging.getLogger("batch_logger")
logger.setLevel(logging.INFO)
logger.propagate = False  # é˜²æ­¢é‡å¤è¾“å‡ºåˆ° stdout

# æ¸…é™¤å·²æœ‰çš„ handlersï¼ˆé˜²æ­¢å¤šæ¬¡è¿è¡Œè„šæœ¬æ—¶é‡å¤æ·»åŠ ï¼‰
if logger.hasHandlers():
    logger.handlers.clear()

# æ–‡ä»¶ handler
file_handler = logging.FileHandler(LOG_FILE, mode='w')  # mode='a' è¡¨ç¤ºè¿½åŠ å†™å…¥
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(file_handler)

# æ§åˆ¶å° handler
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(console_handler)

# é”™è¯¯ handler
error_handler = logging.FileHandler(ERROR_LOG_FILE, mode='w')
error_handler.setLevel(logging.WARNING)
error_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(error_handler)


PROMPT = """# ğŸ” è§’è‰²ï¼šAIå†…å®¹é£é™©ç®¡ç†ä¸æ–‡æœ¬åˆ†ç±»ä¸“å®¶

## ğŸ§  ç®€ä»‹
- **è¯­è¨€èƒ½åŠ›**ï¼šå¤šè¯­è¨€æ”¯æŒ  
- **èŒè´£æ¦‚è¿°**ï¼šè¯†åˆ«å¹¶åˆ†ç±»æ¶‰åŠAIæŠ€æœ¯ï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶ã€è¯­è¨€æ¨¡å‹ã€æœºå™¨äººã€æ— äººæœºã€æ·±åº¦å­¦ä¹ ç­‰ï¼‰çš„é£é™©æ€§å†…å®¹ã€‚åˆ†ææ•°æ®éšç§ã€æ¥æºåˆæ³•æ€§ã€ç®—æ³•åè§ã€åˆè§„é—®é¢˜ã€æ³•å¾‹ä¼¦ç†ç­‰æ–¹é¢çš„æ½œåœ¨é£é™©ã€‚
- **ä¸“ä¸šèƒŒæ™¯**ï¼šè®¡ç®—æœºç§‘å­¦å­¦ä½ï¼Œè¾…ä¿®ä¼¦ç†å­¦ï¼›å…·å¤‡AIé£é™©è¯„ä¼°ã€ä¼¦ç†å®¡æŸ¥ä¸åˆè§„å®¡è®¡ç»éªŒï¼›æ›¾å‚ä¸AIä¼¦ç†ä¸é£é™©ç ”ç©¶é¡¹ç›®ã€‚
- **ä¸ªæ€§ç‰¹å¾**ï¼šä¸¥è°¨ã€æ³¨é‡ç»†èŠ‚ã€åˆ†æåŠ›å¼ºï¼›è‡´åŠ›äºæ¨åŠ¨AIæŠ€æœ¯çš„è´Ÿè´£ä»»éƒ¨ç½²ã€‚
- **ç›®æ ‡ç”¨æˆ·**ï¼šAIå¼€å‘è€…ã€å†…å®¹å®¡æ ¸å‘˜ã€æ”¿ç­–åˆ¶å®šè€…ã€æŠ€æœ¯å…¬å¸ç­‰ã€‚

## ğŸ§© æ ¸å¿ƒæŠ€èƒ½

### 1. AIé£é™©è¯†åˆ«
- ç²¾å‡†è¯†åˆ«AIç›¸å…³å†…å®¹ä¸­çš„æ³•å¾‹ã€ä¼¦ç†ã€éšç§ç­‰é£é™©å› ç´ ã€‚
- å®¡æ ¸æ•°æ®ä½¿ç”¨çš„åˆæ³•æ€§ä¸åˆè§„æ€§ã€‚

### 2. æ–‡æœ¬åˆ†ç±»
- åˆ¤å®šæ–‡æœ¬æ˜¯å¦ä¸AIé£é™©ç›´æ¥ç›¸å…³ã€‚
- è¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å®ç°è‡ªåŠ¨åˆ†ç±»ã€‚

## ğŸ“‹ åˆ¤æ–­è§„åˆ™

### 1. è¾“å…¥ç»“æ„
- æ¯é¡¹å†…å®¹åŒ…å«â€œæ ‡é¢˜â€åŠâ€œéƒ¨åˆ†æ­£æ–‡â€ã€‚

### 2. ç›¸å…³æ€§æ ‡å‡†
- æ˜ç¡®æåŠAIæŠ€æœ¯ï¼ˆå¦‚è¯­è¨€æ¨¡å‹ã€è‡ªåŠ¨é©¾é©¶ã€æ·±åº¦å­¦ä¹ ç­‰ï¼‰ã€‚
- å†…å®¹éœ€æ¶‰åŠå…·ä½“é£é™©ï¼šéšç§æ³„éœ²ã€åè§æ­§è§†ã€æ³•å¾‹è´£ä»»ã€æ•°æ®åˆè§„ã€è·¨å¢ƒé£é™©ã€ä¼¦ç†é“å¾·é—®é¢˜ç­‰ã€‚
- æœªå…·ä½“æŒ‡æ¶‰é£é™©çš„å†…å®¹ï¼ˆå¦‚æ³›æ³›æè¿°AIåº”ç”¨ï¼‰è§†ä¸ºæ— å…³ã€‚

### 3. æ’é™¤æ ‡å‡†
- æ— å…³ä¸»é¢˜ï¼ˆå¦‚æ™®é€šæ•™è‚²ã€åŒ»ç–—ã€ç»æµæŠ¥é“ä¸­æ³›æŒ‡AIçš„å†…å®¹ï¼‰ã€‚
- å†…å®¹æœªæ¶‰åŠAIç›¸å…³é£é™©ï¼Œä»…ä¸ºæŠ€æœ¯æè¿°æˆ–æ³›ç”¨åœºæ™¯ã€‚

## âš™ï¸ å·¥ä½œæµç¨‹

### ğŸ¯ ç›®æ ‡
åˆ¤æ–­æ¥æ”¶çš„5æ¡æ–‡ç« æ˜¯å¦ä¸AIé£é™©**ç›´æ¥ç›¸å…³**ã€‚

### ğŸ” æ­¥éª¤
1. æ¥æ”¶åŒ…å«æ ‡é¢˜ä¸æ­£æ–‡çš„æ–‡ç« åˆ—è¡¨ã€‚
2. å¯¹æ¯é¡¹è¿›è¡Œå†…å®¹åˆ†æï¼š
   - æ˜¯å¦æåŠAIæŠ€æœ¯ï¼›
   - æ˜¯å¦æ˜ç¡®åŒ…å«é£é™©ç‚¹ï¼ˆéšç§ã€åè§ã€æ³•å¾‹ã€ä¼¦ç†ç­‰ï¼‰ã€‚
3. æ ¹æ®åˆ¤æ–­è¾“å‡ºåˆ†ç±»ç»“æœ,å¦‚æœä¸AIé£é™©**ç›´æ¥ç›¸å…³**ï¼Œè¾“å‡º"AIGCrisk_relevant",å¦‚æœä¸ç›¸å…³ï¼Œè¾“å‡º"AIGCrisk_Irrelevant"ã€‚
### ğŸ“¤ è¾“å…¥æ ¼å¼
- ä¸€ä¸ªjsonæ ¼å¼çš„å­—ç¬¦ä¸²
- jsonå­—ç¬¦ä¸²ä¸­åŒ…å«5ä¸ªå¯¹è±¡ï¼Œè¡¨ç¤º5ç¯‡æ–‡ç« ã€‚
- å…¶ä¸­æ¯ä¸ªå¯¹è±¡åŒ…å«ä¸¤ä¸ªå­—æ®µï¼š
  - `"title"`ï¼šæ–‡ç« æ ‡é¢˜
  - `"content"`ï¼šæ–‡ç« æ­£æ–‡
####ç¤ºä¾‹è¾“å…¥ï¼š
- '[{"title": "æ ‡é¢˜1", "content": "æ­£æ–‡1"}, 
{"title": "æ ‡é¢˜2", "content": "æ­£æ–‡2"},
{"title": "æ ‡é¢˜3", "content": "æ­£æ–‡3"},
{"title": "æ ‡é¢˜4", "content": "æ­£æ–‡4"},
{"title": "æ ‡é¢˜5", "content": "æ­£æ–‡5"}]'
### ğŸ“¤ è¾“å‡ºæ ¼å¼
- ä»…è¿”å›ä¸€ä¸ªjsonæ ¼å¼çš„å­—ç¬¦ä¸²
- è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­åŒ…å«5ä¸ªå­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºæ¯ç¯‡æ–‡ç« çš„åˆ†ç±»ç»“æœã€‚
- å›ç­”ç»“æœçš„æ¯ä¸€é¡¹æœ‰ä¸”åªæœ‰ä¸¤ç§å›ç­”.1)`"AIGCrisk_relevant"`ï¼šæ˜ç¡®æ¶‰åŠAIé£é™©ã€‚2)`"AIGCrisk_Irrelevant"`ï¼šä¸æ¶‰åŠAIé£é™©æˆ–è¡¨è¿°æ¨¡ç³Šä¸æ˜ã€‚
- è¿”å›ç»“æœçš„åˆ—è¡¨å…ƒç´ ä¸ªæ•°åº”è¯¥ä¸è¾“å…¥çš„åˆ—è¡¨å…ƒç´ æ•°é‡ä¸€è‡´ï¼ï¼ï¼ï¼ï¼ï¼
- è¿”å›ç»“æœçš„åˆ—è¡¨å›ç­”çš„é¡ºåºåº”è¯¥ä¸è¾“å…¥çš„åˆ—è¡¨å…ƒç´ é¡ºåºä¸€è‡´ï¼Œå³å›ç­”ç»“æœçš„ç¬¬ä¸€é¡¹å¯¹åº”è¾“å…¥çš„ç¬¬ä¸€é¡¹ï¼Œç¬¬äºŒé¡¹å¯¹åº”ç¬¬äºŒé¡¹ï¼Œä»¥æ­¤ç±»æ¨ã€‚
#### ç¤ºä¾‹è¾“å‡ºï¼š
["AIGCrisk_relevant", "AIGCrisk_Irrelevant", "AIGCrisk_Irrelevant", "AIGCrisk_Irrelevant", "AIGCrisk_Irrelevant"]

##æ³¨æ„äº‹é¡¹
- åªè¿”å›ä¸€ä¸ªjsonæ ¼å¼çš„å­—ç¬¦ä¸²åˆ—è¡¨
- ä»…è¿”å›åˆ†ç±»ç»“æœï¼Œä¸è¦è¿”å›ä»»ä½•é¢å¤–çš„æ–‡æœ¬æˆ–è§£é‡Šã€‚
- ç¡®ä¿è¾“å‡ºæ ¼å¼æ­£ç¡®ï¼Œé¿å…å¤šä½™çš„ç©ºæ ¼æˆ–æ¢è¡Œã€‚
- å¦‚æœæ— æ³•åˆ¤æ–­ï¼Œè¯·è¿”å› `"AIGCrisk_Irrelevant"`ã€‚
""" 


def extract_title(html: str) -> str:
    try:
        soup = BeautifulSoup(html, 'html.parser')
        return soup.title.text.strip() if soup.title else ""
    except Exception as e:
        logger.warning(f"æ ‡é¢˜æå–å¤±è´¥: {e}")
        return ""

def get_txt_summary(txt_path: Path) -> (str, str):
    try:
        text = txt_path.read_text(encoding='utf-8', errors='ignore').strip()
        start = time.time()
        if len(text) < 100:
            return "", text
        inputs = tokenizer([text.replace("\n", " ")], max_length=1024, return_tensors="pt", truncation=True).to(DEVICE)
        summary_ids = model.generate(inputs["input_ids"], max_length=150, min_length=80, length_penalty=2.0, num_beams=4, early_stopping=True)
        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        end = time.time()
        logger.info(f"[æ‘˜è¦] {txt_path.name} æ€»è€—æ—¶ï¼š{end - start:.2f} ç§’")
        return summary, text
    except Exception as e:
        logger.warning(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
        return "", ""

async def detect_ai_risk_batches(text_batches, system_prompt=PROMPT):
    def fetch_model_response(texts):
        global global_inference_counter
        try:
            formatted_input = texts  # è¿™é‡Œæ˜¯ JSON æ ¼å¼çš„æ–‡æœ¬
            thinking,response,tokens= call_model(system_prompt, formatted_input)#å°†æ€è€ƒå’Œæ­£å¼å›ç­”åˆ†å¼€
            # è®¡æ•°æ¨ç†æ¬¡æ•°
            global_inference_counter += 1
            if global_inference_counter % 1000 == 0:
                logger.info(f"ğŸš¦ å·²æ¨ç† {global_inference_counter} æ¬¡ï¼Œä¼‘çœ  120 ç§’ä»¥é˜²æ­¢å µå¡...")
                time.sleep(120)  # ç”¨ time.sleep è¿™é‡Œå°±å¤Ÿäº†ï¼Œå› ä¸ºæ˜¯åœ¨åŒæ­¥éƒ¨åˆ†
            return thinking,response,tokens
        except Exception as e:
            logger.error(f"ğŸ›‘ æœ¬åœ°æ¨¡å‹è¯·æ±‚å¤±è´¥: {e}")
            return (["AIGCrisk_Irrelevant"] * len(texts), 0)

    async def fetch(texts):
        for attempt in range(3):
            try:
                start=time.time()
                thinking,response_str,tokens= fetch_model_response(texts)
                end=time.time()
                logger.info(f"[Model] æ‰¹æ¬¡æ¨ç†å®Œæˆ,ç”¨æ—¶{end-start:.2f}ç§’")
                #print(f"response_str: 1{response_str}2")
                if not response_str:
                    logger.warning(f"âš ï¸ å“åº”ä¸ºç©ºæˆ–ç¼ºå°‘ 'response' å­—æ®µ:")
                    return (["AIGCrisk_Irrelevant"] * len(texts), 0)

                if isinstance(response_str, str):
                    try:
                        start_idx = response_str.find('[')
                        end_idx = response_str.rfind(']')
                        if start_idx != -1 and end_idx != -1:
                            list_str = response_str[start_idx:end_idx + 1]
                            if list_str.startswith('"') and list_str.endswith('"'):
                                list_str = list_str[1:-1]
                            response_list = json.loads(list_str)
                            return response_list,tokens
                        else:
                            logger.warning(f"âš ï¸ å“åº”ä¸­ç¼ºå°‘ä¸­æ‹¬å·: {response_str[:]}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ å“åº”è§£æå¤±è´¥: {e}\nåŸå§‹ response å­—ç¬¦ä¸²: {response_str[:]}")
            except TimeoutError as e:
                logger.error(f"â±ï¸ API è¯·æ±‚è¶…æ—¶ï¼ˆç¬¬ {attempt+1} æ¬¡ï¼‰: {e}")
            except Exception as e:
                logger.error(f"ğŸ›‘ æœªçŸ¥é”™è¯¯ï¼ˆç¬¬ {attempt+1} æ¬¡ï¼‰: {type(e).__name__}: {e}")
            await asyncio.sleep(1)
        return (["AIGCrisk_Irrelevant"] * len(texts), 0)
    tasks = [fetch(batch) for batch in text_batches]
    return await asyncio.gather(*tasks)
# å°†è¾“å…¥æ ¼å¼è½¬æ¢ä¸º JSONï¼Œç»“æ„åŒ–çš„å‘é€ç»™æ¨¡å‹
def prepare_input_for_model(batch_data):
    # æ ¼å¼åŒ–è¾“å…¥ä¸º JSONï¼Œæ¯ä¸ªæ–°é—»ä¸ºä¸€ä¸ªå¯¹è±¡ï¼ŒåŒ…æ‹¬æ ‡é¢˜å’Œæ­£æ–‡
    input_data = [{"title": title, "content": content} for _, title, content in batch_data]
    return json.dumps(input_data, ensure_ascii=False)

async def handle_batch(batch_data, output_dir):
    #æ‰¹æ¬¡æ¨ç†ä¸æ–‡ä»¶å¤„ç†
    texts = prepare_input_for_model(batch_data)
    results_and_tokens = await detect_ai_risk_batches([texts])
    relevant_count = 0
    results, total_tokens = results_and_tokens[0]
    for (fpath, title, original_txt), result in zip(batch_data, results):
        logger.info(f"æ–‡ä»¶å·²å¤„ç†: {fpath.stem}.txt | ç»“æœ: {result}")
    # è®°å½•æœ¬æ‰¹æ¬¡çš„ tokens
    logger.info(f"æœ¬æ‰¹æ¬¡ä½¿ç”¨tokens: {total_tokens}")
    for (fpath, title, original_txt), result in zip(batch_data, results):
        if result == 'AIGCrisk_relevant':
            try:
                #print(f"è·¯å¾„{fpath}")
                combined = f"{title}\n{original_txt}"[:CONTENT_LIMIT]
                out_path = output_dir / f"{fpath.stem}.txt"
                #print(f"è¾“å‡ºè·¯å¾„: {out_path}")
                if out_path.exists():
                    logger.warning(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ‹’ç»è¦†ç›–: {out_path}")
                    continue
                else:
                    out_path.write_text(combined, encoding="utf-8", errors="ignore")

                logger.info(f"âœ… ä¿å­˜é£é™©æ–‡æœ¬: {fpath.stem}.txt ")
                relevant_count += 1
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜å¤±è´¥: {e}")
    return relevant_count,total_tokens

async def main():
    start = time.time()
    total_files = 0
    relevant_files = 0
    total_tokens_per_batch = {5: []}  # ç”¨äºå­˜å‚¨ä¸åŒæ‰¹æ¬¡çš„tokensæ•°é‡
    #Path(SUMMARY_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
    #print("1")
    ym_dirs = [Path(BASE_INPUT_DIR) / ym   for ym in SELECTED_YM if (Path(BASE_INPUT_DIR) / ym ).is_dir()]
    for ym_dir in ym_dirs:
        #print("1")
        txt_files = list(ym_dir.glob("*.txt"))
        if not txt_files:
            
            continue

        #txt_files = random.sample(txt_files, min(len(txt_files), 3600))
        #ç”¨äºéšæœºæŠ½å–æ–‡ä»¶è¿›è¡Œç­›é€‰çš„æµ‹è¯•

        logger.info(f"å¤„ç†ç›®å½•: {ym_dir} éšæœºæŠ½å– {len(txt_files)} ä¸ªæ–‡ä»¶")

        rel_path = ym_dir.relative_to(BASE_INPUT_DIR)
        #print(f"ç›¸å¯¹è·¯å¾„: {rel_path}")
        output_dir = Path(BASE_OUTPUT_DIR) / rel_path
        #print(f"è¾“å‡ºç›®å½•: {output_dir}")
        output_dir.mkdir(parents=True, exist_ok=True)

        batch_data = []
        for batch_size in [5]:  # ä¾æ¬¡æµ‹è¯•æ‰¹æ¬¡å¤§å°ä¸º5, 10, 15çš„æƒ…å†µ  è¿™é‡Œæ˜¯åœ¨å°è¯•æµ‹é‡æ‰¹æ¬¡å¯¹æ¨¡å‹æ¨ç†çš„å½±å“ï¼Œå¯ä»¥ä¸ç”¨æ›´æ”¹
            logger.info(f"å¼€å§‹å¤„ç†æ‰¹æ¬¡å¤§å°: {batch_size}")
            for txt_path in txt_files:
                try:
                    with open(txt_path, 'r', encoding='utf-8') as f:
                        title = f.readline().strip()
                        content = f.read().strip()


                    batch_data.append((txt_path,title,content))
                    logger.info(f"æ·»åŠ åˆ°æ‰¹æ¬¡: {txt_path.name} | æ‰¹æ¬¡å¤§å°: {len(batch_data)}")
                    if len(batch_data) >= batch_size:
                        relevant_count, batch_tokens = await handle_batch(batch_data, output_dir)
                        relevant_files += relevant_count
                        total_files += len(batch_data)
                        total_tokens_per_batch[batch_size].append(batch_tokens)  # è®°å½•æ¯ä¸ªæ‰¹æ¬¡çš„tokensæ•°é‡
                        batch_data.clear()  # æ¸…ç©ºbatch_dataï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ªæ‰¹æ¬¡

                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {txt_path}: {e}")

            # å¤„ç†å‰©ä½™çš„æ–‡ä»¶
            if batch_data:
                relevant_count, batch_tokens = await handle_batch(batch_data, output_dir)
                relevant_files += relevant_count
                total_files += len(batch_data)
                total_tokens_per_batch[batch_size].append(batch_tokens)  
    # è®¡ç®—æ¯ç§æ‰¹æ¬¡å¤§å°çš„å¹³å‡tokens
    for batch_size, tokens_list in total_tokens_per_batch.items():
        avg_tokens = np.mean(tokens_list) if tokens_list else 0
        logger.info(f"æ‰¹æ¬¡å¤§å° {batch_size}: å¹³å‡ä½¿ç”¨ {avg_tokens:.2f} tokens")
    logger.info(f"ğŸ•’ æ€»è€—æ—¶ï¼š{time.time() - start:.2f}s")
    if total_files > 0:
        rate = 100 * relevant_files / total_files
        logger.info(f"ğŸ“Š æ€»å…±å¤„ç† {total_files} ç¯‡æ–°é—»ï¼Œé£é™©ç›¸å…³ {relevant_files} ç¯‡ï¼Œæ¯”ä¾‹ {rate:.2f}%")
    else:
        logger.info("ğŸ“­ æœªå¤„ç†ä»»ä½•æ–‡ä»¶")

if __name__ == "__main__":

    asyncio.run(main())
