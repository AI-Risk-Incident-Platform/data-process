import os
import time
import json
import logging
import asyncio
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import numpy as np
from callmodel_doubao import call_model, detect_ai_risk_batches  # è±†åŒ…APIè°ƒç”¨æ¨¡å—
# æ³¨æ„æ ¹æ®å®žé™…è°ƒç”¨çš„APIä¿®æ”¹å¯¼å…¥è·¯å¾„å’Œå‡½æ•°å

# çŽ¯å¢ƒå˜é‡è®¾ç½®
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# é…ç½®å‚æ•°
BASE_INPUT_DIR = "/data3/downloads/cc-news"  # è¾“å…¥æ ¹ç›®å½•
BASE_OUTPUT_DIR = "/data3/downloads/cc-news-risks_batch2_notthink"  # è¾“å‡ºæ ¹ç›®å½•
LOG_FILE = "batch_processing_2025_5-6month.log"
ERROR_LOG_FILE = "error.log"
CONTENT_LIMIT = 10000
SELECTED_YM = ["2025/5", "2025/6"]  # éœ€å¤„ç†çš„å¹´æœˆ
import threading  # ç”¨äºŽçº¿ç¨‹å®‰å…¨çš„è®¡æ•°å™¨

# çŽ¯å¢ƒå˜é‡å’Œé…ç½®å‚æ•°ä¸å˜ï¼ˆä»…ä¿®æ”¹MAX_PROCESSESä¸ºMAX_THREADSï¼‰
MAX_THREADS = 5  # çº¿ç¨‹æ•°ï¼ˆå¯æ ¹æ®APIå¹¶å‘é™åˆ¶è°ƒæ•´ï¼Œå»ºè®®5-10ï¼‰
BATCH_SIZE = 5  # æ¯æ‰¹å¤„ç†5ä¸ªæ–‡ä»¶ï¼ˆä¸Žæç¤ºè¯ä¸€è‡´ï¼‰

# æ—¥å¿—é…ç½®
def setup_logger():
    logger = logging.getLogger("batch_logger")
    logger.setLevel(logging.INFO)
    logger.propagate = False

    if logger.hasHandlers():
        logger.handlers.clear()

    # æ–‡ä»¶å’ŒæŽ§åˆ¶å°æ—¥å¿—
    file_handler = logging.FileHandler(LOG_FILE, mode='w')
    console_handler = logging.StreamHandler()
    error_handler = logging.FileHandler(ERROR_LOG_FILE, mode='w')

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(processName)s - %(message)s')
    for handler in [file_handler, console_handler, error_handler]:
        handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    logger.addHandler(error_handler)

    return logger

logger = setup_logger()

# æç¤ºè¯
PROMPT = """# ðŸ” è§’è‰²ï¼šAIå†…å®¹é£Žé™©ç®¡ç†ä¸Žæ–‡æœ¬åˆ†ç±»ä¸“å®¶

## ðŸ§  ç®€ä»‹
- **è¯­è¨€èƒ½åŠ›**ï¼šå¤šè¯­è¨€æ”¯æŒ Â 
- **èŒè´£æ¦‚è¿°**ï¼šè¯†åˆ«å¹¶åˆ†ç±»æ¶‰åŠAIæŠ€æœ¯ï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶ã€è¯­è¨€æ¨¡åž‹ã€æœºå™¨äººã€æ— äººæœºã€æ·±åº¦å­¦ä¹ ç­‰ï¼‰çš„é£Žé™©æ€§å†…å®¹ã€‚åˆ†æžæ•°æ®éšç§ã€æ¥æºåˆæ³•æ€§ã€ç®—æ³•åè§ã€åˆè§„é—®é¢˜ã€æ³•å¾‹ä¼¦ç†ç­‰æ–¹é¢çš„æ½œåœ¨é£Žé™©ã€‚
- **ä¸“ä¸šèƒŒæ™¯**ï¼šè®¡ç®—æœºç§‘å­¦å­¦ä½ï¼Œè¾…ä¿®ä¼¦ç†å­¦ï¼›å…·å¤‡AIé£Žé™©è¯„ä¼°ã€ä¼¦ç†å®¡æŸ¥ä¸Žåˆè§„å®¡è®¡ç»éªŒï¼›æ›¾å‚ä¸ŽAIä¼¦ç†ä¸Žé£Žé™©ç ”ç©¶é¡¹ç›®ã€‚
- **ä¸ªæ€§ç‰¹å¾**ï¼šä¸¥è°¨ã€æ³¨é‡ç»†èŠ‚ã€åˆ†æžåŠ›å¼ºï¼›è‡´åŠ›äºŽæŽ¨åŠ¨AIæŠ€æœ¯çš„è´Ÿè´£ä»»éƒ¨ç½²ã€‚
- **ç›®æ ‡ç”¨æˆ·**ï¼šAIå¼€å‘è€…ã€å†…å®¹å®¡æ ¸å‘˜ã€æ”¿ç­–åˆ¶å®šè€…ã€æŠ€æœ¯å…¬å¸ç­‰ã€‚

## ðŸ§© æ ¸å¿ƒæŠ€èƒ½

### 1. AIé£Žé™©è¯†åˆ«
- ç²¾å‡†è¯†åˆ«AIç›¸å…³å†…å®¹ä¸­çš„æ³•å¾‹ã€ä¼¦ç†ã€éšç§ç­‰é£Žé™©å› ç´ ã€‚
- å®¡æ ¸æ•°æ®ä½¿ç”¨çš„åˆæ³•æ€§ä¸Žåˆè§„æ€§ã€‚

### 2. æ–‡æœ¬åˆ†ç±»
- åˆ¤å®šæ–‡æœ¬æ˜¯å¦ä¸ŽAIé£Žé™©ç›´æŽ¥ç›¸å…³ã€‚
- è¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å®žçŽ°è‡ªåŠ¨åˆ†ç±»ã€‚

## ðŸ“‹ åˆ¤æ–­è§„åˆ™

### 1. è¾“å…¥ç»“æž„
- æ¯é¡¹å†…å®¹åŒ…å«â€œæ ‡é¢˜â€åŠâ€œéƒ¨åˆ†æ­£æ–‡â€ã€‚

### 2. ç›¸å…³æ€§æ ‡å‡†
- æ˜Žç¡®æåŠAIæŠ€æœ¯ï¼ˆå¦‚è¯­è¨€æ¨¡åž‹ã€è‡ªåŠ¨é©¾é©¶ã€æ·±åº¦å­¦ä¹ ç­‰ï¼‰ã€‚
- å†…å®¹éœ€æ¶‰åŠå…·ä½“é£Žé™©ï¼šéšç§æ³„éœ²ã€åè§æ­§è§†ã€æ³•å¾‹è´£ä»»ã€æ•°æ®åˆè§„ã€è·¨å¢ƒé£Žé™©ã€ä¼¦ç†é“å¾·é—®é¢˜ç­‰ã€‚
- æœªå…·ä½“æŒ‡æ¶‰é£Žé™©çš„å†…å®¹ï¼ˆå¦‚æ³›æ³›æè¿°AIåº”ç”¨ï¼‰è§†ä¸ºæ— å…³ã€‚

### 3. æŽ’é™¤æ ‡å‡†
- æ— å…³ä¸»é¢˜ï¼ˆå¦‚æ™®é€šæ•™è‚²ã€åŒ»ç–—ã€ç»æµŽæŠ¥é“ä¸­æ³›æŒ‡AIçš„å†…å®¹ï¼‰ã€‚
- å†…å®¹æœªæ¶‰åŠAIç›¸å…³é£Žé™©ï¼Œä»…ä¸ºæŠ€æœ¯æè¿°æˆ–æ³›ç”¨åœºæ™¯ã€‚

## âš™ å·¥ä½œæµç¨‹

### ðŸŽ¯ ç›®æ ‡
åˆ¤æ–­æŽ¥æ”¶çš„5æ¡æ–‡ç« æ˜¯å¦ä¸ŽAIé£Žé™©**ç›´æŽ¥ç›¸å…³**ã€‚

### ðŸ”Ž æ­¥éª¤
1. æŽ¥æ”¶åŒ…å«æ ‡é¢˜ä¸Žæ­£æ–‡çš„æ–‡ç« åˆ—è¡¨ã€‚
2. å¯¹æ¯é¡¹è¿›è¡Œå†…å®¹åˆ†æžï¼š
Â  Â - æ˜¯å¦æåŠAIæŠ€æœ¯ï¼›
Â  Â - æ˜¯å¦æ˜Žç¡®åŒ…å«é£Žé™©ç‚¹ï¼ˆéšç§ã€åè§ã€æ³•å¾‹ã€ä¼¦ç†ç­‰ï¼‰ã€‚
3. æ ¹æ®åˆ¤æ–­è¾“å‡ºåˆ†ç±»ç»“æžœ,å¦‚æžœä¸ŽAIé£Žé™©**ç›´æŽ¥ç›¸å…³**ï¼Œè¾“å‡º"AIGCrisk_relevant",å¦‚æžœä¸ç›¸å…³ï¼Œè¾“å‡º"AIGCrisk_Irrelevant"ã€‚
### ðŸ“¤ è¾“å…¥æ ¼å¼
- ä¸€ä¸ªjsonæ ¼å¼çš„å­—ç¬¦ä¸²
- jsonå­—ç¬¦ä¸²ä¸­åŒ…å«5ä¸ªå¯¹è±¡ï¼Œè¡¨ç¤º5ç¯‡æ–‡ç« ã€‚
- å…¶ä¸­æ¯ä¸ªå¯¹è±¡åŒ…å«ä¸¤ä¸ªå­—æ®µï¼š
Â  - `"title"`ï¼šæ–‡ç« æ ‡é¢˜
Â  - `"content"`ï¼šæ–‡ç« æ­£æ–‡
####ç¤ºä¾‹è¾“å…¥ï¼š
- '[{"title": "æ ‡é¢˜1", "content": "æ­£æ–‡1"}, 
{"title": "æ ‡é¢˜2", "content": "æ­£æ–‡2"},
{"title": "æ ‡é¢˜3", "content": "æ­£æ–‡3"},
{"title": "æ ‡é¢˜4", "content": "æ­£æ–‡4"},
{"title": "æ ‡é¢˜5", "content": "æ­£æ–‡5"}]'
### ðŸ“¤ è¾“å‡ºæ ¼å¼
- ä»…è¿”å›žä¸€ä¸ªjsonæ ¼å¼çš„å­—ç¬¦ä¸²
- è¿”å›žä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­åŒ…å«5ä¸ªå­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºæ¯ç¯‡æ–‡ç« çš„åˆ†ç±»ç»“æžœã€‚
- å›žç­”ç»“æžœçš„æ¯ä¸€é¡¹æœ‰ä¸”åªæœ‰ä¸¤ç§å›žç­”.1)`"AIGCrisk_relevant"`ï¼šæ˜Žç¡®æ¶‰åŠAIé£Žé™©ã€‚2)`"AIGCrisk_Irrelevant"`ï¼šä¸æ¶‰åŠAIé£Žé™©æˆ–è¡¨è¿°æ¨¡ç³Šä¸æ˜Žã€‚
- è¿”å›žç»“æžœçš„åˆ—è¡¨å…ƒç´ ä¸ªæ•°åº”è¯¥ä¸Žè¾“å…¥çš„åˆ—è¡¨å…ƒç´ æ•°é‡ä¸€è‡´ï¼ï¼ï¼ï¼ï¼ï¼
- è¿”å›žç»“æžœçš„åˆ—è¡¨å›žç­”çš„é¡ºåºåº”è¯¥ä¸Žè¾“å…¥çš„åˆ—è¡¨å…ƒç´ é¡ºåºä¸€è‡´ï¼Œå³å›žç­”ç»“æžœçš„ç¬¬ä¸€é¡¹å¯¹åº”è¾“å…¥çš„ç¬¬ä¸€é¡¹ï¼Œç¬¬äºŒé¡¹å¯¹åº”ç¬¬äºŒé¡¹ï¼Œä»¥æ­¤ç±»æŽ¨ã€‚
#### ç¤ºä¾‹è¾“å‡ºï¼š
["AIGCrisk_relevant", "AIGCrisk_Irrelevant", "AIGCrisk_Irrelevant", "AIGCrisk_Irrelevant", "AIGCrisk_Irrelevant"]

##æ³¨æ„äº‹é¡¹
- åªè¿”å›žä¸€ä¸ªjsonæ ¼å¼çš„å­—ç¬¦ä¸²åˆ—è¡¨
- ä»…è¿”å›žåˆ†ç±»ç»“æžœï¼Œä¸è¦è¿”å›žä»»ä½•é¢å¤–çš„æ–‡æœ¬æˆ–è§£é‡Šã€‚
- ç¡®ä¿è¾“å‡ºæ ¼å¼æ­£ç¡®ï¼Œé¿å…å¤šä½™çš„ç©ºæ ¼æˆ–æ¢è¡Œã€‚
- å¦‚æžœæ— æ³•åˆ¤æ–­ï¼Œè¯·è¿”å›ž `"AIGCrisk_Irrelevant"`ã€‚
""" 

def prepare_input_for_model(batch_data):
    """æ ¼å¼åŒ–è¾“å…¥ä¸º5æ¡æ–‡ç« çš„JSONåˆ—è¡¨"""
    input_data = [{"title": title, "content": content} for _, title, content in batch_data]
    return json.dumps(input_data, ensure_ascii=False)

async def process_single_batch(batch_data, output_root):
    """å¤„ç†å•ä¸ªæ‰¹æ¬¡ï¼Œè¾“å‡ºè·¯å¾„ä¸å«textsæ–‡ä»¶å¤¹"""
    try:
        texts = prepare_input_for_model(batch_data)
        results_and_tokens = await detect_ai_risk_batches([texts], PROMPT)
        relevant_count = 0
        results, total_tokens = results_and_tokens[0]

        if len(results) != len(batch_data):
            logger.warning(f"ç»“æžœé•¿åº¦ä¸åŒ¹é…ï¼Œé¢„æœŸ{len(batch_data)}ï¼Œå®žé™…{len(results)}")
            return 0, total_tokens

        for (fpath, title, original_txt), result in zip(batch_data, results):
            if result == 'AIGCrisk_relevant':
                try:
                    # å…³é”®è°ƒæ•´ï¼šè¾“å‡ºè·¯å¾„åŽ»æŽ‰textså±‚çº§
                    # è¾“å…¥æ–‡ä»¶è·¯å¾„ç¤ºä¾‹ï¼šBASE_INPUT_DIR/2022/5/texts/news_001.txt
                    parent_dir = fpath.parent  # å¾—åˆ°"BASE_INPUT_DIR/2022/5/texts"
                    year_month_dir = parent_dir.parent  # å¾—åˆ°"BASE_INPUT_DIR/2022/5"
                    rel_path = year_month_dir.relative_to(BASE_INPUT_DIR)  # å¾—åˆ°"2022/5"
                    output_dir = Path(output_root) / rel_path  # è¾“å‡ºç›®å½•ï¼šBASE_OUTPUT_DIR/2022/5
                    output_dir.mkdir(parents=True, exist_ok=True)
                    
                    out_path = output_dir / f"{fpath.stem}.txt"
                    if not out_path.exists():
                        combined = f"{title}\n{original_txt}"[:CONTENT_LIMIT]
                        out_path.write_text(combined, encoding="utf-8", errors="ignore")
                        relevant_count += 1
                except Exception as e:
                    logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥ {fpath}: {e}")

        return relevant_count, total_tokens
    except Exception as e:
        logger.error(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
        return 0, 0

def worker_thread(batch, output_root, counter_lock, global_counter):
    """çº¿ç¨‹å·¥ä½œå‡½æ•°ï¼ˆæ›¿ä»£åŽŸworker_processï¼‰"""
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        relevant_count, tokens = loop.run_until_complete(
            process_single_batch(batch, output_root)
        )
        loop.close()
        
        # çº¿ç¨‹å®‰å…¨æ›´æ–°è®¡æ•°å™¨ï¼ˆç”¨threading.Lockï¼‰
        with counter_lock:
            global_counter[0] += 1  # ç”¨åˆ—è¡¨å­˜å‚¨è®¡æ•°å™¨ï¼Œå®žçŽ°å¯å˜å¯¹è±¡å…±äº«
            current = global_counter[0]
            if current % 100 == 0:  # æ¯100æ‰¹æ¬¡è¾“å‡ºä¸€æ¬¡è¿›åº¦ï¼ˆæ›´é¢‘ç¹ç›‘æŽ§ï¼‰
                logger.info(f"å…¨å±€å·²å¤„ç† {current} æ‰¹æ¬¡")
            if current % 1000 == 0:
                logger.info(f"å…¨å±€å·²å¤„ç† {current} æ‰¹æ¬¡ï¼Œä¼‘çœ 120ç§’...")
                time.sleep(120)
                
        return relevant_count, len(batch), tokens
    except Exception as e:
        logger.error(f"çº¿ç¨‹å¤„ç†å¤±è´¥: {e}")
        return 0, len(batch), 0

def main():
    start = time.time()
    
    # 1. æ”¶é›†æ–‡ä»¶ï¼ˆé€»è¾‘ä¸å˜ï¼‰
    all_files = []
    for ym in SELECTED_YM:
        ym_dir = Path(BASE_INPUT_DIR) / ym
        if not ym_dir.is_dir():
            logger.warning(f"å¹´æœˆç›®å½•ä¸å­˜åœ¨ï¼š{ym_dir}")
            continue
        
        texts_dir = ym_dir / "texts"
        if not texts_dir.exists() or not texts_dir.is_dir():
            logger.info(f"æ— textsæ–‡ä»¶å¤¹ï¼Œè·³è¿‡ï¼š{ym_dir}")
            continue
        
        txt_files = list(texts_dir.rglob("*.txt"))  # é€’å½’æŸ¥æ‰¾
        all_files.extend(txt_files)
        logger.info(f"ä»Ž {texts_dir} æ”¶é›†åˆ° {len(txt_files)} ä¸ªæ–‡ä»¶")
    
    if not all_files:
        logger.info("æœªæ‰¾åˆ°ä»»ä½•txtæ–‡ä»¶ï¼Œé€€å‡º")
        return
    
    logger.info(f"å…±æ”¶é›† {len(all_files)} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ {MAX_THREADS} çº¿ç¨‹å¤„ç†")
    
    # 2. åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆä¸å˜ï¼‰
    output_root = Path(BASE_OUTPUT_DIR)
    output_root.mkdir(parents=True, exist_ok=True)
    
    # 3. åˆ’åˆ†æ‰¹æ¬¡ï¼ˆä¸å˜ï¼‰
    batches = []
    for i in range(0, len(all_files), BATCH_SIZE):
        batch_files = all_files[i:i+BATCH_SIZE]
        batch_data = []
        for fpath in batch_files:
            try:
                with open(fpath, 'r', encoding='utf-8') as f:
                    title = f.readline().strip()
                    content = f.read().strip()
                batch_data.append((fpath, title, content))
            except Exception as e:
                logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {fpath}: {e}")
        if batch_data:
            batches.append(batch_data)
    
    logger.info(f"æ–‡ä»¶å·²åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡ï¼ˆæ¯æ‰¹æœ€å¤š{5}ä¸ªï¼‰")
    
    # 4. å¤šçº¿ç¨‹å¤„ç†ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
    global_counter = [0]  # ç”¨åˆ—è¡¨å®žçŽ°çº¿ç¨‹é—´å…±äº«çš„è®¡æ•°å™¨ï¼ˆå¯å˜å¯¹è±¡ï¼‰
    counter_lock = threading.Lock()  # çº¿ç¨‹å®‰å…¨çš„é”
    
    # ç”¨ThreadPoolExecutoræ›¿ä»£Pool
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        # æž„å»ºçº¿ç¨‹ä»»åŠ¡å‚æ•°ï¼ˆä¼ é€’é”å’Œè®¡æ•°å™¨ï¼‰
        worker_args = [
            (batch, output_root, counter_lock, global_counter) 
            for batch in batches
        ]
        # æäº¤æ‰€æœ‰ä»»åŠ¡å¹¶èŽ·å–ç»“æžœ
        results = list(executor.map(
            lambda x: worker_thread(x[0], x[1], x[2], x[3]), 
            worker_args
        ))
    
    # 5. æ±‡æ€»ç»“æžœï¼ˆä¸å˜ï¼‰
    total_relevant = 0
    total_processed = 0
    total_tokens = 0
    for rel, processed, tokens in results:
        total_relevant += rel
        total_processed += processed
        total_tokens += tokens
    
    # è¾“å‡ºç»Ÿè®¡ï¼ˆä¸å˜ï¼‰
    duration = time.time() - start
    logger.info(f"\n===== å¤„ç†å®Œæˆ =====")
    logger.info(f"æ€»è€—æ—¶: {duration:.2f}ç§’")
    logger.info(f"æ€»å¤„ç†æ–‡ä»¶: {total_processed}")
    logger.info(f"é£Žé™©ç›¸å…³æ–‡ä»¶: {total_relevant}")
    if total_processed > 0:
        logger.info(f"ç›¸å…³æ¯”ä¾‹: {100 * total_relevant / total_processed:.2f}%")
    logger.info(f"æ€»Tokenæ¶ˆè€—: {total_tokens}")

if __name__ == "__main__":
    main()