import time
import logging
import os
import json
import asyncio
from typing import Tuple, Optional
from openai import OpenAI

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("doubao_model_calls.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def call_model(
    system_prompt: str, 
    user_prompt: str,
    model_name: str = "doubao-1-5-pro-32k-250115",
    temperature: float = 0.7,
    max_retries: int = 3
) -> Tuple[Optional[str], Optional[str], Optional[int]]:
    """
    è°ƒç”¨è±†åŒ…APIè¿›è¡ŒAIé£é™©å…³é”®è¯ç­›é€‰
    
    å‚æ•°:
        system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼Œå®šä¹‰æ¨¡å‹è§’è‰²å’Œä»»åŠ¡
        user_prompt: ç”¨æˆ·è¾“å…¥çš„å¾…å¤„ç†å†…å®¹
        model_name: æ¨¡å‹åç§°
        temperature: ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶è¾“å‡ºéšæœºæ€§
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    è¿”å›:
        thinking_part: æ¨¡å‹æ€è€ƒè¿‡ç¨‹
        answer_part: æ¨¡å‹æ­£å¼å›ç­”
        tokens: æ¶ˆè€—çš„tokenæ•°é‡
    """
    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®æˆ–ä½¿ç”¨é»˜è®¤å€¼
    llm_url = os.getenv("LLM_URL", "https://ark.cn-beijing.volces.com/api/v3")
    api_key = os.getenv("API_KEY", "28be7d5d-b9c9-4cc8-8ac8-528aa0870790")

    # é…ç½®æ£€æŸ¥
    if api_key == "28be7d5d-b9c9-4cc8-8ac8-528aa0870790":
        logger.warning("ä½¿ç”¨é»˜è®¤APIå¯†é’¥ï¼Œå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œï¼Œè¯·æ›¿æ¢ä¸ºå®é™…å¯†é’¥")
    
    client = OpenAI(
        base_url=llm_url,
        api_key=api_key
    )

    for attempt in range(max_retries):
        try:
            start_time = time.time()
            completion = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
                top_p=0.8,
                max_tokens=4096
            )
            
            response_time = time.time() - start_time
            tokens_used = completion.usage.total_tokens if completion.usage else 0
            logger.info(
                f"è±†åŒ…APIè°ƒç”¨æˆåŠŸ (å°è¯• {attempt+1}/{max_retries}) "
                f"| è€—æ—¶: {response_time:.2f}ç§’ "
                f"| Tokenæ¶ˆè€—: {tokens_used}"
            )

            content = completion.choices[0].message.content
            
            # è±†åŒ…æ¨¡å‹å¯èƒ½ä¸ä½¿ç”¨ç‰¹å®šæ€è€ƒæ ‡è®°ï¼Œå› æ­¤å°†æ‰€æœ‰å†…å®¹è§†ä¸ºå›ç­”
            return None, content.strip(), tokens_used

        except Exception as err:
            logger.error(
                f"è±†åŒ…APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}) "
                f"| é”™è¯¯ç±»å‹: {type(err).__name__} "
                f"| é”™è¯¯ä¿¡æ¯: {str(err)}"
            )
            
            # é‡è¯•å‰ç­‰å¾…ï¼ŒæŒ‡æ•°é€€é¿ç­–ç•¥
            if attempt < max_retries - 1:
                sleep_time = (attempt + 1) * 2  # 1st: 2s, 2nd: 4s
                logger.info(f"å°†åœ¨ {sleep_time} ç§’åé‡è¯•...")
                time.sleep(sleep_time)

    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    logger.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œè°ƒç”¨å¤±è´¥")
    return None, None, None

# æ‰¹é‡å¤„ç†å‡½æ•°ï¼Œé€‚é…ä¸»ç¨‹åº
async def detect_ai_risk_batches(text_batches, system_prompt):
    """å¤„ç†æ‰¹é‡æ–‡æœ¬ï¼Œè°ƒç”¨è±†åŒ…APIè¿›è¡ŒAIé£é™©æ£€æµ‹"""
    def fetch_model_response(texts):
        try:
            # è°ƒç”¨è±†åŒ…æ¨¡å‹
            thinking, response_str, tokens = call_model(system_prompt, texts)
            return thinking, response_str, tokens
        except Exception as e:
            logger.error(f"æ¨¡å‹è¯·æ±‚å¤±è´¥: {e}")
            return (["AIGCrisk_Irrelevant"] * len(json.loads(texts)), 0)

    async def fetch(texts):
        for attempt in range(3):
            try:
                start = time.time()
                thinking, response_str, tokens = fetch_model_response(texts)
                end = time.time()
                logger.info(f"[è±†åŒ…æ¨¡å‹] æ‰¹æ¬¡æ¨ç†å®Œæˆ,ç”¨æ—¶{end-start:.2f}ç§’")

                if not response_str:
                    logger.warning(f"å“åº”ä¸ºç©º: {response_str}")
                    return (["AIGCrisk_Irrelevant"] * len(json.loads(texts)), 0)

                # è§£æè±†åŒ…è¿”å›çš„JSONç»“æœ
                try:
                    # æŸ¥æ‰¾JSONæ•°ç»„çš„å¼€å§‹å’Œç»“æŸä½ç½®
                    start_idx = response_str.find('[')
                    end_idx = response_str.rfind(']')
                    if start_idx != -1 and end_idx != -1:
                        list_str = response_str[start_idx:end_idx + 1]
                        response_list = json.loads(list_str)
                        return response_list, tokens
                    else:
                        logger.warning(f"å“åº”ä¸­ç¼ºå°‘æœ‰æ•ˆçš„JSONæ•°ç»„: {response_str[:100]}...")
                except Exception as e:
                    logger.warning(f"å“åº”è§£æå¤±è´¥: {e}\nåŸå§‹å“åº”: {response_str[:100]}...")
            except Exception as e:
                logger.error(f"å¤„ç†å¤±è´¥ï¼ˆç¬¬ {attempt+1} æ¬¡ï¼‰: {e}")
            await asyncio.sleep(1)
        # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æœ
        return (["AIGCrisk_Irrelevant"] * len(json.loads(texts)), 0)

    tasks = [fetch(batch) for batch in text_batches]
    return await asyncio.gather(*tasks)

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # æµ‹è¯•è±†åŒ…APIè°ƒç”¨
    test_system = """# ğŸ” è§’è‰²ï¼šAIå†…å®¹é£é™©ç®¡ç†ä¸æ–‡æœ¬åˆ†ç±»ä¸“å®¶

## ğŸ§  ç®€ä»‹
- **è¯­è¨€èƒ½åŠ›**ï¼šå¤šè¯­è¨€æ”¯æŒ Â 
- **èŒè´£æ¦‚è¿°**ï¼šè¯†åˆ«å¹¶åˆ†ç±»æ¶‰åŠAIæŠ€æœ¯ï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶ã€è¯­è¨€æ¨¡å‹ã€æœºå™¨äººã€æ— äººæœºã€æ·±åº¦å­¦ä¹ ç­‰ï¼‰çš„é£é™©æ€§å†…å®¹ã€‚åˆ†ææ•°æ®éšç§ã€æ¥æºåˆæ³•æ€§ã€ç®—æ³•åè§ã€åˆè§„é—®é¢˜ã€æ³•å¾‹ä¼¦ç†ç­‰æ–¹é¢çš„æ½œåœ¨é£é™©ã€‚
- **ä¸“ä¸šèƒŒæ™¯**ï¼šè®¡ç®—æœºç§‘å­¦å­¦ä½ï¼Œè¾…ä¿®ä¼¦ç†å­¦ï¼›å…·å¤‡AIé£é™©è¯„ä¼°ã€ä¼¦ç†å®¡æŸ¥ä¸åˆè§„å®¡è®¡ç»éªŒï¼›æ›¾å‚ä¸AIä¼¦ç†ä¸é£é™©ç ”ç©¶é¡¹ç›®ã€‚
- **ä¸ªæ€§ç‰¹å¾**ï¼šä¸¥è°¨ã€æ³¨é‡ç»†èŠ‚ã€åˆ†æåŠ›å¼ºï¼›è‡´åŠ›äºæ¨åŠ¨AIæŠ€æœ¯çš„è´Ÿè´£ä»»éƒ¨ç½²ã€‚
- **ç›®æ ‡ç”¨æˆ·**ï¼šAIå¼€å‘è€…ã€å†…å®¹å®¡æ ¸å‘˜ã€æ”¿ç­–åˆ¶å®šè€…ã€æŠ€æœ¯å…¬å¸ç­‰ã€‚

## ğŸ§© æ ¸å¿ƒæŠ€èƒ½

### 1. AIé£é™©è¯†åˆ«
- ç²¾å‡†è¯†åˆ«AIç›¸å…³å†…å®¹ä¸­çš„æ³•å¾‹ã€ä¼¦ç†ã€éšç§ç­‰é£é™©å› ç´ ã€‚
- å®¡æ ¸æ•°æ®ä½¿ç”¨çš„åˆæ³•æ€§ä¸åˆè§„æ€§ã€‚

### 2. æ–‡æœ¬åˆ†ç±»
- åˆ¤å®šæ–‡æœ¬æ˜¯å¦ä¸AIé£é™©ç›´æ¥ç›¸å…³ã€‚
- è¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å®ç°è‡ªåŠ¨åˆ†ç±»ã€‚

## ğŸ“‹ åˆ¤æ–­è§„åˆ™

### 1. è¾“å…¥ç»“æ„
- æ¯é¡¹å†…å®¹åŒ…å«â€œæ ‡é¢˜â€åŠâ€œéƒ¨åˆ†æ­£æ–‡â€ã€‚

### 2. ç›¸å…³æ€§æ ‡å‡†
- æ˜ç¡®æåŠAIæŠ€æœ¯ï¼ˆå¦‚è¯­è¨€æ¨¡å‹ã€è‡ªåŠ¨é©¾é©¶ã€æ·±åº¦å­¦ä¹ ç­‰ï¼‰ã€‚
- å†…å®¹éœ€æ¶‰åŠå…·ä½“é£é™©ï¼šéšç§æ³„éœ²ã€åè§æ­§è§†ã€æ³•å¾‹è´£ä»»ã€æ•°æ®åˆè§„ã€è·¨å¢ƒé£é™©ã€ä¼¦ç†é“å¾·é—®é¢˜ç­‰ã€‚
- æœªå…·ä½“æŒ‡æ¶‰é£é™©çš„å†…å®¹ï¼ˆå¦‚æ³›æ³›æè¿°AIåº”ç”¨ï¼‰è§†ä¸ºæ— å…³ã€‚

### 3. æ’é™¤æ ‡å‡†
- æ— å…³ä¸»é¢˜ï¼ˆå¦‚æ™®é€šæ•™è‚²ã€åŒ»ç–—ã€ç»æµæŠ¥é“ä¸­æ³›æŒ‡AIçš„å†…å®¹ï¼‰ã€‚
- å†…å®¹æœªæ¶‰åŠAIç›¸å…³é£é™©ï¼Œä»…ä¸ºæŠ€æœ¯æè¿°æˆ–æ³›ç”¨åœºæ™¯ã€‚

## âš™ å·¥ä½œæµç¨‹

### ğŸ¯ ç›®æ ‡
åˆ¤æ–­æ¥æ”¶çš„5æ¡æ–‡ç« æ˜¯å¦ä¸AIé£é™©**ç›´æ¥ç›¸å…³**ã€‚

### ğŸ” æ­¥éª¤
1. æ¥æ”¶åŒ…å«æ ‡é¢˜ä¸æ­£æ–‡çš„æ–‡ç« åˆ—è¡¨ã€‚
2. å¯¹æ¯é¡¹è¿›è¡Œå†…å®¹åˆ†æï¼š
Â  Â - æ˜¯å¦æåŠAIæŠ€æœ¯ï¼›
Â  Â - æ˜¯å¦æ˜ç¡®åŒ…å«é£é™©ç‚¹ï¼ˆéšç§ã€åè§ã€æ³•å¾‹ã€ä¼¦ç†ç­‰ï¼‰ã€‚
3. æ ¹æ®åˆ¤æ–­è¾“å‡ºåˆ†ç±»ç»“æœ,å¦‚æœä¸AIé£é™©**ç›´æ¥ç›¸å…³**ï¼Œè¾“å‡º"AIGCrisk_relevant",å¦‚æœä¸ç›¸å…³ï¼Œè¾“å‡º"AIGCrisk_Irrelevant"ã€‚
### ğŸ“¤ è¾“å…¥æ ¼å¼
- ä¸€ä¸ªjsonæ ¼å¼çš„å­—ç¬¦ä¸²
- jsonå­—ç¬¦ä¸²ä¸­åŒ…å«5ä¸ªå¯¹è±¡ï¼Œè¡¨ç¤º5ç¯‡æ–‡ç« ã€‚
- å…¶ä¸­æ¯ä¸ªå¯¹è±¡åŒ…å«ä¸¤ä¸ªå­—æ®µï¼š
Â  - `"title"`ï¼šæ–‡ç« æ ‡é¢˜
Â  - `"content"`ï¼šæ–‡ç« æ­£æ–‡
####ç¤ºä¾‹è¾“å…¥ï¼š
- '[{"title": "æ ‡é¢˜1", "content": "æ­£æ–‡1"}, 
{"title": "æ ‡é¢˜2", "content": "æ­£æ–‡2"},
{"title": "æ ‡é¢˜3", "content": "æ­£æ–‡3"},
{"title": "æ ‡é¢˜4", "content": "æ­£æ–‡4"},
{"title": "æ ‡é¢˜5", "content": "æ­£æ–‡5"}]'
### ğŸ“¤ è¾“å‡ºæ ¼å¼
- ä»…è¿”å›ä¸€ä¸ªjsonæ ¼å¼çš„å­—ç¬¦ä¸²
- è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­åŒ…å«5ä¸ªå­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºæ¯ç¯‡æ–‡ç« çš„åˆ†ç±»ç»“æœã€‚
- å›ç­”ç»“æœçš„æ¯ä¸€é¡¹æœ‰ä¸”åªæœ‰ä¸¤ç§å›ç­”.1)`"AIGCrisk_relevant"`ï¼šæ˜ç¡®æ¶‰åŠAIé£é™©ã€‚2)`"AIGCrisk_Irrelevant"`ï¼šä¸æ¶‰åŠAIé£é™©æˆ–è¡¨è¿°æ¨¡ç³Šä¸æ˜ã€‚
- è¿”å›ç»“æœçš„åˆ—è¡¨å…ƒç´ ä¸ªæ•°åº”è¯¥ä¸è¾“å…¥çš„åˆ—è¡¨å…ƒç´ æ•°é‡ä¸€è‡´ï¼ï¼ï¼ï¼ï¼ï¼
- è¿”å›ç»“æœçš„åˆ—è¡¨å›ç­”çš„é¡ºåºåº”è¯¥ä¸è¾“å…¥çš„åˆ—è¡¨å…ƒç´ é¡ºåºä¸€è‡´ï¼Œå³å›ç­”ç»“æœçš„ç¬¬ä¸€é¡¹å¯¹åº”è¾“å…¥çš„ç¬¬ä¸€é¡¹ï¼Œç¬¬äºŒé¡¹å¯¹åº”ç¬¬äºŒé¡¹ï¼Œä»¥æ­¤ç±»æ¨ã€‚
#### ç¤ºä¾‹è¾“å‡ºï¼š
["AIGCrisk_relevant", "AIGCrisk_Irrelevant", "AIGCrisk_Irrelevant", "AIGCrisk_Irrelevant", "AIGCrisk_Irrelevant"]

##æ³¨æ„äº‹é¡¹
- åªè¿”å›ä¸€ä¸ªjsonæ ¼å¼çš„å­—ç¬¦ä¸²åˆ—è¡¨
- ä»…è¿”å›åˆ†ç±»ç»“æœï¼Œä¸è¦è¿”å›ä»»ä½•é¢å¤–çš„æ–‡æœ¬æˆ–è§£é‡Šã€‚
- ç¡®ä¿è¾“å‡ºæ ¼å¼æ­£ç¡®ï¼Œé¿å…å¤šä½™çš„ç©ºæ ¼æˆ–æ¢è¡Œã€‚
- å¦‚æœæ— æ³•åˆ¤æ–­ï¼Œè¯·è¿”å› `"AIGCrisk_Irrelevant"`ã€‚
""" 
    test_user = json.dumps([
        {"title": "AIç®—æ³•æ­§è§†é—®é¢˜ç ”ç©¶", "content": "æœ¬æ–‡è®¨è®ºäº†AIç®—æ³•ä¸­å­˜åœ¨çš„æ­§è§†é—®é¢˜åŠå…¶ç¤¾ä¼šå½±å“"},
        {"title": "æ™®é€šè®¡ç®—æœºæŠ€æœ¯å‘å±•", "content": "æœ¬æ–‡ä»‹ç»äº†ä¼ ç»Ÿè®¡ç®—æœºæŠ€æœ¯çš„å‘å±•å†ç¨‹"},
        {"title": "ç”Ÿæˆå¼AIçš„ç‰ˆæƒäº‰è®®", "content": "å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®æ¶‰åŠå¤§é‡æœªç»æˆæƒçš„ä½œå“ï¼Œå¼•å‘æ³•å¾‹çº çº·"},
        {"title": "è‡ªåŠ¨é©¾é©¶å®‰å…¨äº‹æ•…åˆ†æ", "content": "æŸå“ç‰Œè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå› ç®—æ³•ç¼ºé™·å¯¼è‡´äº¤é€šäº‹æ•…"},
        {"title": "Pythonç¼–ç¨‹å…¥é—¨æ•™ç¨‹", "content": "ä»‹ç»PythonåŸºç¡€è¯­æ³•å’Œå˜é‡å®šä¹‰"}
    ], ensure_ascii=False)
    
    async def test():
        # è°ƒç”¨æ¨¡å‹è·å–ç»“æœ
        results = await detect_ai_risk_batches([test_user], test_system)
        parsed_results, tokens = results[0]
        
        # æ‰“å°æ¨¡å‹åŸå§‹è¾“å‡ºï¼ˆå…³é”®æ–°å¢éƒ¨åˆ†ï¼‰
        print("\n===== æ¨¡å‹åŸå§‹è¾“å‡º =====")
        # é‡æ–°è°ƒç”¨ä¸€æ¬¡æ¨¡å‹ï¼Œä¸“é—¨æ‰“å°åŸå§‹å“åº”
        thinking, raw_response, _ = call_model(test_system, test_user)
        print(raw_response)
        
        # æ‰“å°è§£æåçš„ç»“æœ
        print("\n===== è§£æåçš„ç»“æœ =====")
        print(parsed_results)
        print(f"\nTokenæ¶ˆè€—: {tokens}")
    
    asyncio.run(test())